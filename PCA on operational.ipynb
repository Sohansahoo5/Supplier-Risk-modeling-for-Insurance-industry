{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3be6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "\n",
    "# Read data from Excel file\n",
    "data = pd.read_excel(\"Operations_Dataset_Dummy.xlsx\")\n",
    "\n",
    "# Clean up column names\n",
    "data.columns = [re.sub(r'\\W+', '', col) for col in data.columns]\n",
    "\n",
    "# Separate 'Company' column and feature columns\n",
    "companies = data['Company']\n",
    "features = data.drop(columns=['Company'])\n",
    "\n",
    "# Step 1: Standardize the feature data\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Fit the PCA model\n",
    "pca = PCA(n_components=features.shape[1])  # Set the number of components equal to the number of features\n",
    "pca.fit(features_standardized)\n",
    "\n",
    "# Step 3: Extract the principal components and their corresponding weights\n",
    "principal_components = pca.components_\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate average weights for each feature\n",
    "average_weights = np.abs(principal_components).mean(axis=0) * np.sqrt(explained_variance_ratio)\n",
    "\n",
    "# Normalize weights so that the sum equals 1\n",
    "normalized_weights = average_weights / np.sum(average_weights)\n",
    "\n",
    "# Define weights for each variable based on feature importance scores\n",
    "weights = {}\n",
    "for i, column in enumerate(features.columns):\n",
    "    weights[column] = normalized_weights[i]\n",
    "\n",
    "# Define conditional mappings for compliance scoring based on variable values\n",
    "conditional_mappings = {\n",
    "    'FragilityStateIndex': lambda value: 1 - value / features['FragilityStateIndex'].max(),  # Inverse relationship with fragility state index\n",
    "    'AvgNaturalDisasterFrequencyperyear': lambda value: 1 - value / features['AvgNaturalDisasterFrequencyperyear'].max(),  # Inverse relationship with natural disaster frequency\n",
    "    'GeographicDiversity': lambda value: 1 if value == 1 else 0,\n",
    "    'DataOwnershipandReturn': lambda value: 1 if value == 1 else 0,\n",
    "    'BusinessContinuityPlans': lambda value: 1 if value == 1 else 0,\n",
    "    'TransitionAssistance': lambda value: 1 if value == 1 else 0,\n",
    "    'SingleSourcing': lambda value: 1 if value == 1 else 0,\n",
    "    'DisasterRecoveryPlans': lambda value: 1 if value == 1 else 0,\n",
    "    'RedundancyandFailoverMechanisms': lambda value: 1 if value == 1 else 0,\n",
    "    'IncidentResponseandCrisisManagement': lambda value: 1 if value == 1 else 0\n",
    "}\n",
    "\n",
    "# Calculate operations score for each company\n",
    "operations_scores = {}\n",
    "for i, row in data.iterrows():\n",
    "    company = row['Company']\n",
    "    score = 0\n",
    "    for column in features.columns:\n",
    "        # Apply conditional mapping to adjust operations score based on variable value\n",
    "        score += weights[column] * conditional_mappings[column](row[column])\n",
    "    # Ensure the score is a single value\n",
    "    operations_scores[company] = score\n",
    "\n",
    "# Reverse and scale the scores\n",
    "scaled_operations_scores = {company: (1 - score) * 100 for company, score in operations_scores.items()}\n",
    "\n",
    "# Create a DataFrame from the scaled_compliance_scores dictionary\n",
    "scaled_operations_scores_df = pd.DataFrame(list(scaled_operations_scores.items()), columns=['Company', 'Scaled operations Score'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "scaled_operations_scores_df.to_csv(\"scaled_operations_scores_24mar.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2447986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FragilityStateIndex': 0.13548697891936906,\n",
       " 'AvgNaturalDisasterFrequencyperyear': 0.12220487142103865,\n",
       " 'GeographicDiversity': 0.12552415088145924,\n",
       " 'DataOwnershipandReturn': 0.11373965854460093,\n",
       " 'BusinessContinuityPlans': 0.11164390149084703,\n",
       " 'TransitionAssistance': 0.10204833853541168,\n",
       " 'SingleSourcing': 0.08949102152195829,\n",
       " 'DisasterRecoveryPlans': 0.07308592018692822,\n",
       " 'RedundancyandFailoverMechanisms': 0.07314689665973557,\n",
       " 'IncidentResponseandCrisisManagement': 0.053628261838651305}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961b007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
